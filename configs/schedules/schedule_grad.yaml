optimizer_config:
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 2e-4
      weight_decay: 1e-3
  lr_scheduler:
    scheduler:
      class_path: torch.optim.lr_scheduler.ExponentialLR
      init_args:
        gamma: 0.88
    # scheduler:
    #   class_path: torch.optim.lr_scheduler.MultiStepLR
    #   init_args:
    #     milestones: [8, 11]
    # warmup_config:
    #   warmup_iters: 600

trainer:
  max_epochs: 1
